# VoiceQuery_LLM_Response

# Overview
The objective was to create a pipeline that takes a voice query command, converts it into text, uses a Large Language Model (LLM) to generate a response, and then converts the output text back into speech.

The pipeline was built using the following main components:
1. Speech-to-Text: Implemented using the Faster-Whisper model to transcribe voice command into text.
2. LLM for Response Generation: Implemented using Hugging Face's gpt2 model to generate responses.
3. Text-to-Speech: Implemented using Google TTS library to convert the text back into speech.

# Explanation

# Step1: Audio Recording
The task was done using Google Colab since it uses a cloud based environment, however the basic version was used and so the computational resources were limited and the codes were run on a CPU. The access to the local microphone via Colab was a bit complex so the code from the
source: https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be was used, which makes use of JavaScript to access the microphone. The code to record audio was modified to generate the audio according to the faster-whisper model’s compatibility.

• The decoded audio data is converted into an AudioSegment object using the pydub library, which allows for manipulation of the audio data. The audio format specified here is webm, which is a common format for web-based audio recordings.
• if audio.channels > 1: Checks if the audio has more than one channel. If true, it reduces the audio to a single channel to simplify further processing.
• samples = np.array(audio.get_array_of_samples(), dtype=np.float32): Converts the AudioSegment object into a NumPy array.
• samples = samples / np.abs(samples).max(): This normalises the audio numpy array between the values of [-1,1] to ensure consistency across audio segments.

# Speech to Text
Source: https://github.com/SYSTRAN/faster-whisper?tab=readme-ov-file
Faster-Whisper model was selected because it is up to 4 times faster than OpenAI whisper for the same accuracy while using less memory.
The transcribe method is used to convert the audio into text segments.

Parameters:
• vad_filter=True: Voice Activity Detection (VAD) is enabled as required. It helps the model focus on parts of the audio with speech, ignoring silence or noise.
• vad_parameters={"threshold": 0.5}: The VAD threshold determines the sensitivity of the voice activity detection and is set to 0.5 based on this https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py,
• language="en": The language is set to English.

The function then iterates through each segment and prints the start and end time of the transcription along with the transcribed text. The function then concatenates all the transcribed segments into a single string to form the complete transcription.

# Step 2: Generating The Response From an LLM

These classes are from the Hugging Face Transformers library and are used to load the tokenizer and the pre-trained GPT-2 model. GPT-2 was selected because it is a compact model that balances performance and resource usage. Although there were issues faced in employing this
model which will be discussed later, it was the best choice, due to limited resources.

• Tokenizer function tokenises the input text and converts into tensors that can be processed by the model.
• Padding ensures uniform input length, and truncation limits the input length to a maximum of 50 tokens.
• The pad token IDs in the input are replaced with the EOS token IDs to ensure that padding does not interfere with the text generation.
• The attention mask is adjusted to focus on meaningful tokens and ignore padding.
• The model generates text based on the input, with the below parameters to control the output:
    temperature=0.7: Adjusting the response diversity so the response has variety.
    Repetition penalty=1.2: One of the main drawbacks for using gpt2 was the repetition of words hence used this penalty.

One of the problems faced was that the first sentence of the response generated by LLM was the input text to it. So, this code checks if the first sentence of the generated response repeats the input text. If it does, that sentence is removed. The response is limited to two sentences by joining only the first two sentences of the processed output.

# Step3: Converting the Generated Response Text to Speech

In this task, Google-TTS was used to convert the text response generated from the LLM back to speech.
